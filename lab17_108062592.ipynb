{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line make pop-out window not appear\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "\n",
    "game = FlappyBird(pipe_gap = 300)\n",
    "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
    "env.reset_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input Size\n",
    "IMG_WIDTH = 84\n",
    "IMG_HEIGHT = 84\n",
    "NUM_STACK = 4\n",
    "# For Epsilon-greedy\n",
    "MIN_EXPLORING_RATE = 0.01\n",
    "max_step_forward = 10\n",
    "\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(2)\n",
    " \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, num_action, discount_factor=0.99):\n",
    "        self.exploring_rate = 0.1\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_action = num_action\n",
    "        self.actor = Actor()\n",
    "        self.critic = Critic()\n",
    "        \n",
    "        checkpoint_dir = './checkpoint'\n",
    "        \n",
    "        self.actor_ckpt = os.path.join(checkpoint_dir, \"actor_ckpt\")\n",
    "        self.critic_ckpt = os.path.join(checkpoint_dir, \"critic_ckpt\")\n",
    "    \n",
    "    def save_model(self, ep):\n",
    "        self.actor.save(self.actor_ckpt, 'model_{}.h5'.format(ep))\n",
    "        self.critic.save(self.critic_ckpt, 'model_{}.h5'.format(ep))\n",
    "        \n",
    "    def restore_model(self):\n",
    "        self.actor_ckpt.restore(self.actor_manager.latest_checkpoint)\n",
    "        self.critic_ckpt.restore(self.critic_manager.latest_checkpoint)\n",
    "    \n",
    "    def td_target(self, memory, next_state, ternimal):\n",
    "        if ternimal:\n",
    "            v_value = 0.  # terminal\n",
    "        else:\n",
    "            v_value = self.critic(next_state)\n",
    "\n",
    "        # Get discounted rewards\n",
    "        discounted_rewards = []\n",
    "        for reward in memory.rewards[::-1]:  # reverse buffer r\n",
    "            v_value = reward + 0.99*v_value\n",
    "            discounted_rewards.append(v_value)\n",
    "            \n",
    "        discounted_rewards.reverse()\n",
    "\n",
    "        \n",
    "        return discounted_rewards\n",
    "\n",
    "    def advantage(self, td_targets, baselines):\n",
    "        td_targets = tf.convert_to_tensor(np.array(td_targets)[:, None], dtype=tf.float32)\n",
    "        return td_targets - baselines\n",
    "    \n",
    "    def loss(self, memory, next_state, ternimal):\n",
    "        td_target = self.td_target(memory, next_state, ternimal)\n",
    "        advantage = self.advantage(td_target, self.critic(np.vstack(memory.states)))\n",
    "        \n",
    "        critic_loss = advantage**2\n",
    "        \n",
    "        actions_one_hot = tf.one_hot(memory.actions, 2, dtype=tf.float32)\n",
    "        \n",
    "        logits = self.actor(np.vstack(memory.states))\n",
    "        policy = tf.nn.softmax(logits)\n",
    "        entropy = tf.reduce_sum(policy * tf.math.log(policy + 1e-20), axis=1)\n",
    "        \n",
    "        #print(td_target)\n",
    "        #print(logits)\n",
    "        actor_loss = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels=actions_one_hot,\n",
    "                                                                 logits=logits)\n",
    "        actor_loss *= tf.stop_gradient(advantage)\n",
    "        actor_loss -= 0.01 * entropy\n",
    "\n",
    "        total_loss = tf.reduce_mean((0.5 * critic_loss + actor_loss))\n",
    "        return total_loss\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.exploring_rate:\n",
    "            action = np.random.choice(self.num_action)  # Select a random action\n",
    "        else:\n",
    "            state = np.array(state)\n",
    "            state = np.expand_dims(state, axis = 0)\n",
    "            output = self.actor(state)\n",
    "            output = tf.nn.softmax(output)\n",
    "            \n",
    "            action = np.random.choice(2, p=output.numpy()[0])\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def get_state_idx(self, state):\n",
    "        # instead of using absolute position of pipe, use relative position\n",
    "        state = copy.deepcopy(state)\n",
    "        state['next_next_pipe_bottom_y'] -= state['player_y']\n",
    "        state['next_next_pipe_top_y'] -= state['player_y']\n",
    "        state['next_pipe_bottom_y'] -= state['player_y']\n",
    "        state['next_pipe_top_y'] -= state['player_y']\n",
    "\n",
    "        relative_state = list(state.values())\n",
    "        return relative_state\n",
    "    \n",
    "    def update_parameters(self, episode):\n",
    "        self.exploring_rate = max(MIN_EXPLORING_RATE, min(0.5, 0.99**((episode) / 30)))\n",
    "\n",
    "    def shutdown_explore(self):\n",
    "        # make action selection greedy\n",
    "        self.exploring_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init agent\n",
    "num_action = len(env.getActionSet())\n",
    "\n",
    "# agent for frequently updating\n",
    "online_agent = Agent('online', num_action)\n",
    "\n",
    "# agent for slow updating\n",
    "target_agent = Agent('target', num_action)\n",
    "# synchronize target model's weight with online model's weight\n",
    "target_agent.actor.set_weights(online_agent.actor.get_weights())\n",
    "target_agent.critic.set_weights(online_agent.critic.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "average_loss = tf.keras.metrics.Mean(name='loss')\n",
    "\n",
    "#@tf.function\n",
    "def train_step(mem, next_state, ternimal):\n",
    "    # Delayed Target Network\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = online_agent.loss(mem, next_state, ternimal)\n",
    "        \n",
    "    trainable_variables = online_agent.actor.trainable_variables + online_agent.critic.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    average_loss.update_state(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "\n",
    "def make_anim(images, fps=60, true_image=False):\n",
    "    duration = len(images) / fps\n",
    "\n",
    "    def make_frame(t):\n",
    "        try:\n",
    "            x = images[int(len(images) / duration * t)]\n",
    "        except:\n",
    "            x = images[-1]\n",
    "\n",
    "        if true_image:\n",
    "            return x.astype(np.uint8)\n",
    "        else:\n",
    "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "    clip.fps = fps\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.transform\n",
    "\n",
    "def preprocess_screen(screen):\n",
    "    screen = skimage.transform.resize(screen, [IMG_WIDTH, IMG_HEIGHT, 1])\n",
    "    return screen\n",
    "\n",
    "def frames_to_state(input_frames):\n",
    "    if(len(input_frames) == 1):\n",
    "        state = np.concatenate(input_frames*4, axis=-1)\n",
    "    elif(len(input_frames) == 2):\n",
    "        state = np.concatenate(input_frames[0:1]*2 + input_frames[1:]*2, axis=-1)\n",
    "    elif(len(input_frames) == 3):\n",
    "        state = np.concatenate(input_frames + input_frames[2:], axis=-1)\n",
    "    else:\n",
    "        state = np.concatenate(input_frames[-4:], axis=-1)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def store(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(reward, state):\n",
    "    if reward == 0:\n",
    "        reward = 0.1\n",
    "    elif reward == 1:\n",
    "        reward = 100\n",
    "    elif reward == -5:\n",
    "        reward = -500    \n",
    "    '''if state[3] > 5: # player < next_pipe_top_y\n",
    "        reward -= 0.001*abs(state[3])\n",
    "    elif state[4] < -5: # player > next_pipe_bottom_y\n",
    "        reward -= 0.001*abs(state[4])\n",
    "    else:\n",
    "        reward += 1'''\n",
    "    #reward -= abs(state[3] + state[4])\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "update_every_iteration = 1000\n",
    "print_every_episode = 500\n",
    "save_video_every_episode = 500\n",
    "NUM_EPISODE = 2000000\n",
    "NUM_EXPLORE = 20\n",
    "\n",
    "iter_num = 0\n",
    "online_agent.shutdown_explore()\n",
    "mem = Memory()\n",
    "for episode in range(0, NUM_EPISODE + 1):\n",
    "    \n",
    "    # Reset the environment\n",
    "    env.reset_game()\n",
    "    \n",
    "    # record frame\n",
    "    if episode % save_video_every_episode == 0:\n",
    "        frames = [env.getScreenRGB()]\n",
    "    \n",
    "    # input frame\n",
    "    input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
    "    \n",
    "    # for every 500 episodes, shutdown exploration to see the performance of greedy action\n",
    "    if episode % print_every_episode == 0:\n",
    "        online_agent.shutdown_explore()\n",
    "    \n",
    "    # cumulate reward for this episode\n",
    "    cum_reward = 0\n",
    "    \n",
    "    t = 0\n",
    "    mem.clear()\n",
    "    while not env.game_over():\n",
    "        state = game.getGameState()\n",
    "        state = online_agent.get_state_idx(state)\n",
    "        \n",
    "        # feed current state and select an action\n",
    "        action = online_agent.select_action(state)\n",
    "        # execute the action and get reward\n",
    "        reward = env.act(env.getActionSet()[action])\n",
    "        reward = reward_function(reward, state)\n",
    "        # record frame\n",
    "        if episode % save_video_every_episode == 0:\n",
    "            frames.append(env.getScreenRGB())\n",
    "        \n",
    "        # record input frame\n",
    "        input_frames.append(preprocess_screen(env.getScreenGrayscale()))\n",
    "        \n",
    "        # cumulate reward\n",
    "        cum_reward += reward\n",
    "        \n",
    "        # observe the result\n",
    "        state_prime = game.getGameState()\n",
    "        state_prime = online_agent.get_state_idx(state_prime)\n",
    "        \n",
    "        terminal = env.game_over()\n",
    "\n",
    "        # convert Python object to Tensor to prevent graph re-tracing\n",
    "        train_states = np.array([state])\n",
    "        train_states_prime = np.array([state_prime])\n",
    "        \n",
    "        train_states_prime = tf.convert_to_tensor(train_states_prime, tf.float32)\n",
    "        terminal = tf.convert_to_tensor(terminal, tf.bool)\n",
    "        \n",
    "        mem.store(train_states, action, reward)\n",
    "        if t == 25 or terminal:\n",
    "            train_step(mem, train_states_prime, terminal)\n",
    "        \n",
    "        # Setting up for the next iteration\n",
    "        state = state_prime\n",
    "        t += 1\n",
    "    # update exploring rate\n",
    "    #online_agent.update_parameters(episode)\n",
    "\n",
    "    if episode % print_every_episode == 0 and episode > NUM_EXPLORE:\n",
    "        if t >= max_t:\n",
    "            online_agent.save_model(episode)\n",
    "            max_t = t\n",
    "        \n",
    "        print('max_t: ', max_t)\n",
    "        print(\n",
    "            \"[{}] time live:{}, cumulated reward: {}, exploring rate: {}, average loss: {}\".\n",
    "            format(episode, t, cum_reward, online_agent.exploring_rate, average_loss.result()))\n",
    "        average_loss.reset_states()\n",
    "\n",
    "    if episode % save_video_every_episode == 0:  # for every 500 episode, record an animation\n",
    "        clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
    "        clip.write_videofile(\"movie_f/DQN_demo-{}.webm\".format(episode), fps=60)\n",
    "        display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test result\n",
    "env.reset_game()\n",
    "frames = [env.getScreenRGB()]\n",
    "\n",
    "online_agent.shutdown_explore()\n",
    "#online_agent.restore_model()\n",
    "while not env.game_over():\n",
    "    state = game.getGameState()\n",
    "    state = online_agent.get_state_idx(state)\n",
    "    \n",
    "    action = online_agent.select_action(state)\n",
    "    #action = 1\n",
    "    reward = env.act(env.getActionSet()[action])\n",
    "    reward = reward_function(reward, state)\n",
    "    \n",
    "    #print(state[3], state[4])\n",
    "    #print(reward)\n",
    "    \n",
    "    frames.append(env.getScreenRGB())\n",
    "    \n",
    "    state_prime = game.getGameState()\n",
    "    state_prime = online_agent.get_state_idx(state_prime)\n",
    "    state = state_prime\n",
    "    \n",
    "clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
    "clip.write_videofile(\"movie_f/DQN_demo-{}.webm\".format(episode), fps=60)\n",
    "display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset_game()\n",
    "\n",
    "state = game.getGameState()\n",
    "print(state)\n",
    "state = online_agent.get_state_idx(state)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
